---
title: "question3"
author: "Steven Garrity"
date: "10/9/2019"
output: html_document
---

```{r question3, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 3
Address the missing values in each column.
```{r}
n_cols = dim(merged)[2]
for (i in 1:n_cols)
{
  print(c(colnames(merged[i]), sum(is.na(merged[i]))))
}
# It appears that ABV and IBU are the only columns with missing values.

# Extract the row indices of the missing values
missingIBUindex <- which(is.na(merged$IBU))
missingABVindex <- which(is.na(merged$ABV))
allNAindex <- c(missingIBUindex,missingABVindex)
length(missingIBUindex)
length(unique(allNAindex)) 

#try filling ABV values first with the median value by the relevant beer style. In other words, find the style of the beer with a missing value, calculate the median ABV for that style of beer, and fill the missing value with the median for that style.
for (i in 1:length(missingABVindex))
{
  merged$ABV[missingABVindex[i]] <- median(merged$ABV[merged$Style == merged$Style[missingABVindex[i]]],na.rm = TRUE)
}

# sanity check to make sure that we filled all of the missing values:
# which(is.na(merged$ABV)) # looks like all values were filled...we are good to proceed


# Use ABV, Style, Brewery_name, and State as predictor variables in a knn model
forTraining_IBU_kNN <- merged[-missingIBUindex,] %>% 
  select(ABV,Style,Brewery_name,State,IBU) %>%
  mutate(iStyle = as.integer(Style), 
         iBrewery_name = as.integer(Brewery_name), iState = as.integer(State)) %>%
  mutate(abv_z = scale(ABV), style_z = scale(iStyle), 
         brewery_z = scale(iBrewery_name), state_z = scale(iState), IBU = as.factor(IBU)) %>%
  select(abv_z, style_z, brewery_z, state_z, IBU)

forFilling_IBU_kNN <- merged[missingIBUindex,] %>% select(ABV, Style, Brewery_name, State, IBU)

### first tune hyperparameter "k"
splitPerc = .75
iterations = 1
numks = 20

masterAcc = matrix(nrow = iterations, ncol = numks)
  
for(j in 1:iterations)
{
trainIndices = sample(1:dim(forTraining_IBU_kNN)[1],round(splitPerc * dim(forTraining_IBU_kNN)[1]))
train = forTraining_IBU_kNN[trainIndices,]
test = forTraining_IBU_kNN[-trainIndices,]
for(i in 1:numks)
{
  classifications = knn(train[,c(1,4)],test[,c(1,4)],train$IBU, prob = TRUE, k = i)
  table(classifications,test$IBU)
  CM = confusionMatrix(table(classifications,test$IBU))
  masterAcc[j,i] = CM$overall[1]
}

}

MeanAcc = colMeans(masterAcc)
plot(seq(1,numks,1),MeanAcc*100, type = "l", xlab = "k", ylab = "Accuracy (%)")

# find index of k that produces highest accuracy:
which.max(MeanAcc)

xx <- attr(classifications,'levels')

######### Look at a regression tree (CART)
library(rpart)
library(rpart.plot)
# ?rpart

forTraining_IBU_rtree <- merged[-missingIBUindex,] %>% 
  select(ABV,Style,Brewery_name,State,IBU)

forFilling_IBU_rtree <- merged[missingIBUindex,] %>% 
  select(ABV,Style,Brewery_name,State,IBU)

rtree <- rpart(IBU ~ ABV+Style+Brewery_name+State, 
               data = forTraining_IBU_rtree,
               method = "anova",
               control = list(cp = 0, xval = 10))
plotcp(rtree)

# prp(rtree) # visualize the tree

rtree.pred = predict(rtree, forTraining_IBU_rtree[,c(1,2,3,4)])
rtree.sse = sum((rtree.pred - forTraining_IBU_rtree$IBU)^2)
rtree.mse = mean((rtree.pred - forTraining_IBU_rtree$IBU)^2)
rtree.rmse = sqrt(mean((rtree.pred - forTraining_IBU_rtree$IBU)^2))
rtree.sse
rtree.mse
rtree.rmse

plot(rtree.pred,forTraining_IBU_rtree$IBU)
# hmmmm, try to repeat with a train/test split
rtree.pred = predict(rtree, forFilling_IBU_rtree[,c(1,2,3,4)])
rtree.sse = sum((rtree.pred - test$MEDV)^2)
rtree.sse

```

