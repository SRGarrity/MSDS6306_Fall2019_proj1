---
title: "question8"
output: html_document
---

```{r question8, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(class)
library(caret)
library(e1071)
library(pracma)
```

Categorize Ales...
```{r}
# categorize beers as ipa, or other ale
beers_categorized <- beers_raw %>%
  mutate(is_ipa = grepl("\\bIPA\\b",Style,ignore.case = TRUE)) %>%
  mutate(is_ale = grepl("\\bale\\b",Style,ignore.case = TRUE)) %>%
  mutate(is_other_ale = (is_ale & !is_ipa))

# can only work with beers that have valid values,
# and we only care about ipa and other ales
# and make a factor for the type
for_knn <- beers_categorized %>% drop_na(ABV,IBU) %>%
  filter(is_ipa | is_other_ale) %>%
  mutate(category = as.factor(ifelse(is_ipa,'IPA','Other'))) %>%
  mutate(abv_z = scale(ABV), ibu_z = scale(IBU)) %>%
  select(ABV,IBU,abv_z,ibu_z,category)

# columns to use for predictors
knn_pred_indices = c("abv_z","ibu_z")                                                

```

Make a scatter plot of IBU vs ABV, with only IPA's and other ales
```{r}
for_knn %>% ggplot(aes(x=IBU,y=ABV,color=category)) + 
  geom_point() + ggtitle('ABV vs IBU for IPAs and other ales')
```

Do KNN classification
```{r}
splitPerc <- .7
iterations <- 50
numks <- 50
acc <- matrix(nrow=iterations,ncol=numks)
for (j in 1:iterations)
  {
    accs = data.frame(accuracy = numeric(numks), k = numeric(numks))
    trainIndices = sample(1:dim(for_knn)[1],round(splitPerc * dim(for_knn)[1]))
    train = for_knn[trainIndices,]
    test = for_knn[-trainIndices,]
    for(i in 1:numks)
      {
        classifications = knn(train[,knn_pred_indices],
                              test[,knn_pred_indices],
                              as.factor(train$category), prob = TRUE, k = i)
        table(as.factor(test$category),classifications)
        CM = confusionMatrix(table(as.factor(test$category),classifications))
        acc[j,i] = CM$overall[1]
      }
}
mean_acc <- colMeans(acc)
plot(seq(1,numks,1),mean_acc,type="l")
```

```{r}
best_k <- which.max(mean_acc)
max_acc <- max(mean_acc)
best_k
max_acc
```

k of `r best_k` seems to do the best job.

```{r}
splitPerc <- .7
trainIndices = sample(1:dim(for_knn)[1],round(splitPerc * dim(for_knn)[1]))
train = for_knn[trainIndices,]
test = for_knn[-trainIndices,]
classifications = knn(train[,knn_pred_indices],
                      test[,knn_pred_indices],
                      as.factor(train$category), prob = TRUE, k = best_k)
CM = confusionMatrix(table(as.factor(test$category),classifications))
CM
```


```{r}
knn_pred <- knn(for_knn[,knn_pred_indices],
                for_knn[,knn_pred_indices],
                as.factor(for_knn$category),k=best_k)
with_pred <- cbind(for_knn,knn_pred)
with_pred %>% ggplot(aes(x=IBU,y=ABV,color=knn_pred)) + geom_point() + ggtitle('Predicted Class')
```

```{r}
ABV <- linspace(min(for_knn$ABV),max(for_knn$ABV),100)
IBU <- linspace(min(for_knn$IBU),max(for_knn$IBU),100)

dummy <- data.frame(ABV=rep(0,100*100),IBU=rep(0,100*100))
cnt <- 0
for (i in 1:100)
{
  for (j in 1:100)
  {
    cnt <- cnt + 1
    dummy[cnt,] <- c(ABV[i],IBU[j])
  }
}
synth <- dummy %>% mutate(abv_z = scale(ABV), ibu_z = scale(IBU))

synth_pred <- knn(for_knn[,knn_pred_indices],
                  synth[,knn_pred_indices],
                  as.factor(for_knn$category),k=best_k)
synthetic <- cbind(synth,synth_pred)
synthetic %>% ggplot(aes(x=IBU,y=ABV,color=synth_pred)) + geom_point() + ggtitle('Class Map')
```

Consider t-tests of ABV and IBU

First check normality
```{r}
for_knn %>% ggplot(aes(x=ABV,color=category)) +
  geom_histogram(fill="white",alpha=0.5,position="identity") +
  ggtitle('histogram of ABV by category')
```

```{r}
for_knn %>% ggplot(aes(x=IBU,color=category)) +
  geom_histogram(fill="white",alpha=0.5,position="identity") +
  ggtitle('histogram of IBU by category')
```

```{r}
t.test(IBU ~ category,for_knn,var.equal=FALSE)
```

```{r}
t.test(ABV ~ category,for_knn,var.equal=FALSE)
```

